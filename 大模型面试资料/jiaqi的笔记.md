* 自注意力机制为什么是允许模型并行的？

自注意力机制（Self-Attention）是允许模型并行化的关键原因在于它**不依赖于序列中的前后元素顺序**。以下是自注意力机制如何实现并行化的细节：

1. 无依赖关系的计算
在传统的序列模型（如 RNN 或 LSTM）中，每个时刻的输出依赖于前一时刻的输出，因此计算必须按顺序进行，无法并行。然而，自注意力机制通过**一次性计算所有序列位置之间的_相似度_**（即注意力权重），可以同时处理整个序列的所有位置。

2. 矩阵运算并行化
自注意力机制将**输入序列映射为矩阵表示**，然后通过矩阵乘法来计算注意力分数和加权平均。矩阵运算可以使用 GPU 等硬件加速器并行处理，这使得自注意力机制非常高效。具体步骤如下：
   * Query、Key、Value 矩阵的计算：输入序列通过线性变换得到 Query、Key 和 Value 矩阵。这些矩阵的计算是并行的。
   * 注意力权重的计算：通过 Query 和 Key 矩阵的点积计算注意力权重矩阵。这一过程是矩阵乘法操作，可以并行进行。
   * 加权和的计算：注意力权重矩阵与 Value 矩阵相乘，得到最终的输出表示，同样可以并行化。

3. 跨位置的并行处理
自注意力机制的设计使得每个位置的信息可以同时访问其他所有位置的信息，而不需要等待前一个位置的处理完成。因此，无论是训练还是推理，自注意力机制都可以**在一个时间步内处理整个序列**。
