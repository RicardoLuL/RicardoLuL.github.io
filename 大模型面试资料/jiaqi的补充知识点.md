# 补充知识点

## 自注意力机制为什么是允许模型并行的？

自注意力机制（Self-Attention）是允许模型并行化的关键原因在于它**不依赖于序列中的前后元素顺序**。以下是自注意力机制如何实现并行化的细节：

1. 无依赖关系的计算
在传统的序列模型（如 RNN 或 LSTM）中，每个时刻的输出依赖于前一时刻的输出，因此计算必须按顺序进行，无法并行。然而，自注意力机制通过**一次性计算所有序列位置之间的_相似度_**（即注意力权重），可以同时处理整个序列的所有位置。

2. 矩阵运算并行化
自注意力机制将**输入序列映射为矩阵表示**，然后通过矩阵乘法来计算注意力分数和加权平均。矩阵运算可以使用 GPU 等硬件加速器并行处理，这使得自注意力机制非常高效。具体步骤如下：
   * Query、Key、Value 矩阵的计算：输入序列通过线性变换得到 Query、Key 和 Value 矩阵。这些矩阵的计算是并行的。
   * 注意力权重的计算：通过 Query 和 Key 矩阵的点积计算注意力权重矩阵。这一过程是矩阵乘法操作，可以并行进行。
   * 加权和的计算：注意力权重矩阵与 Value 矩阵相乘，得到最终的输出表示，同样可以并行化。

3. 跨位置的并行处理
自注意力机制的设计使得每个位置的信息可以同时访问其他所有位置的信息，而不需要等待前一个位置的处理完成。因此，无论是训练还是推理，自注意力机制都可以**在一个时间步内处理整个序列**。

4. 例如，在机器翻译任务中，Transformer 可以同时关注源语言句子中的所有单词，而不是像 RNN 那样依次处理，从而更好地捕捉单词之间的语义关系和语法结构。
>[!NOTE]
>机器翻译任务中——RNN编码器：将输入序列处理为(x1,x2,...xT),T为句子长度（时间步），每个时间步的隐藏状态ht是由上一个隐藏状态ht-1和当前输入xt决定的；RNN不断更新隐藏状态，直到获得最终的隐藏输入hT，作为解码器的输入
>RNN解码器：解码时候也是每生成输出，利用前一时间步的隐藏状态ht-1和上一步生成的单词yt-1生成当前的隐藏状态ht，然后通过softmax得到当前概率分布
>RNN：每个时间步都只能根据当前输入和上一个时间步的信息更新隐藏状态。
>
>自注意力机制：解码时可以一次性同时关注输入序列中所有的位置信息，计算和每个位置的注意力权重，确定贡献程度，即使句子被打乱，也不会有很大差异（与RNN不同）
