# 多模态预训练模型&&大模型
## CLIP实现原理
CLIP 是由 OpenAI 在 2021 年提出的预训练模型，用于评估给定图像与给定文本描述的匹配程度。该模型使用大量（约 4 亿）从网页中爬取的图像-文本对(pair)数据进行对比学习。

典型的双塔模型，有两个 encoder，一个对应图片，一个对应文本，图像和文本经过各自的 encoder 后，通过简单的点乘来代表不同模态的交互（相似性）。

训练时，假设一个 batch 有 N 对（图像，文本）对，可以有 N x N 种组合方式，对比学习把原始数据集中的 N 个组合作为正样本（下图对角线），把其他的 N x N - N 种组合作为负样本（下图非对角线）
![](./img/
